# Documentation Crawler

Um crawler eficiente e otimizado para baixar documentaÃ§Ãµes de sites e convertÃª-las em um Ãºnico arquivo Markdown com **sumÃ¡rio automÃ¡tico (TOC)**.

## âš™ï¸ Funcionalidades

- ğŸ”— **ResoluÃ§Ã£o robusta de links** (relativos/absolutos) com suporte a mÃºltiplos padrÃµes de documentaÃ§Ã£o
- ğŸŒ **Crawling baseado em domÃ­nio** (crawleia todo o domÃ­nio base, ex: `docs.minimals.cc`)
- ğŸ“‚ **ConversÃ£o para Ãºnico arquivo Markdown** com TOC automÃ¡tico
- ğŸ“ **GeraÃ§Ã£o automÃ¡tica de sumÃ¡rio (Table of Contents)**
- ğŸ›‘ **Timeout e retries** para lidar com falhas de rede
- ğŸ—‚ **Log detalhado** salvo em `crawler.log` para debug
- âš¡ **Threads para aceleraÃ§Ã£o** de downloads paralelos
- ğŸ’¾ **Cache local** para evitar downloads repetidos
- ğŸ¨ **Barra de progresso** com `tqdm`
- ğŸ“Š **EstatÃ­sticas detalhadas** ao final do processo (links encontrados, duplicados, externos, etc.)
- ğŸ§¹ **Filtro conservador** (remove apenas pÃ¡ginas explicitamente inÃºteis: privacy-policy, terms-of-service, etc.)

## ğŸ› ï¸ Requisitos

- **Python 3.7+** (use `python3` no Linux/Mac)
- `pip3` ou `pip` (geralmente vem com o Python)

> âš ï¸ **IMPORTANTE:** Se vocÃª estiver no Linux/Mac, use `python3` e `pip3` em vez de `python` e `pip`.

## ğŸš€ Como Usar

### 1. Clone ou baixe o script

```bash
git clone git@github.com:bychrisr/crawler.git
cd crawler
```

### 2. (Opcional) Crie um ambiente virtual (recomendado)

Um ambiente virtual isola as dependÃªncias do script do resto do seu sistema.

```bash
# Cria o ambiente virtual (pasta 'venv')
python3 -m venv venv

# Ativa o ambiente virtual
# No Linux/Mac:
source venv/bin/activate
# No Windows:
venv\Scripts\activate
```

### 3. Instale as dependÃªncias

```bash
# Se vocÃª estÃ¡ usando ambiente virtual ou Python 3 como padrÃ£o:
pip install requests beautifulsoup4 tqdm lxml

# Se vocÃª precisa especificar Python 3:
pip3 install requests beautifulsoup4 tqdm lxml
```

> **Nota:** O pacote `lxml` Ã© opcional mas recomendado para parsing HTML mais rÃ¡pido.

Ou usando o arquivo `requirements.txt`:

```bash
pip install -r requirements.txt
# ou
pip3 install -r requirements.txt
```

### 4. Execute o script

#### Uso BÃ¡sico

```bash
python3 crawler.py --base-url <URL_DA_DOCUMENTACAO>
```

Este comando usarÃ¡ os **valores padrÃ£o** para todos os outros parÃ¢metros.

#### Exemplos PrÃ¡ticos

**Exemplo 1: DocumentaÃ§Ã£o do Minimals UI**

```bash
python3 crawler.py \
  --base-url https://docs.minimals.cc/introduction/ \
  --output minimals-docs.md \
  --max-pages 100 \
  --workers 3
```

**Exemplo 2: DocumentaÃ§Ã£o do TradingView Lightweight Charts**

```bash
python3 crawler.py \
  --base-url https://tradingview.github.io/lightweight-charts/docs \
  --output tradingview-docs.md \
  --max-pages 200 \
  --workers 4
```

**Exemplo 3: Com limpeza de cache e filtro de conteÃºdo**

```bash
python3 crawler.py \
  --base-url https://docs.exemplo.com/ \
  --output docs.md \
  --max-pages 500 \
  --min-content-length 200 \
  --clear-cache
```

**Exemplo 4: DocumentaÃ§Ã£o grande com mais workers**

```bash
python3 crawler.py \
  --base-url https://vuejs.org/guide/ \
  --output vue-docs.md \
  --max-pages 1000 \
  --workers 5
```

## ğŸ›ï¸ OpÃ§Ãµes de Comando

| Argumento | DescriÃ§Ã£o | Valor PadrÃ£o |
| :--- | :--- | :--- |
| `--base-url` | **(ObrigatÃ³rio)** A URL base da documentaÃ§Ã£o a ser crawleada. | |
| `--output` | Nome do arquivo Markdown de saÃ­da. | `output.md` |
| `--workers` | NÃºmero de threads para downloads paralelos. Mais threads = mais rÃ¡pido, mas cuidado com rate limits. | `2` |
| `--cache-dir` | DiretÃ³rio para armazenar pÃ¡ginas baixadas localmente. | `.cache` |
| `--max-pages` | NÃºmero mÃ¡ximo de pÃ¡ginas a crawlear. | `500` |
| `--min-content-length` | Tamanho mÃ­nimo de conteÃºdo (em caracteres) para considerar uma pÃ¡gina vÃ¡lida. PÃ¡ginas menores sÃ£o descartadas. | `100` |
| `--clear-cache` | Limpa o diretÃ³rio de cache antes de iniciar o crawling. | (flag, nÃ£o tem valor) |

## ğŸ“ Estrutura do Projeto

```
.
â”œâ”€â”€ crawler.py          # Script principal
â”œâ”€â”€ README.md           # Este arquivo
â”œâ”€â”€ requirements.txt    # Lista de dependÃªncias
â”œâ”€â”€ crawler.log         # Log detalhado (gerado automaticamente)
â”œâ”€â”€ output.md           # DocumentaÃ§Ã£o gerada (nome customizÃ¡vel)
â”œâ”€â”€ .cache/            # Cache de pÃ¡ginas HTML (criado automaticamente)
â””â”€â”€ venv/              # (opcional) Ambiente virtual
```

## ğŸ“ Arquivo `requirements.txt`

```txt
requests>=2.31.0
beautifulsoup4>=4.12.0
tqdm>=4.66.0
lxml>=4.9.0
```

Para instalar a partir do arquivo:

```bash
pip3 install -r requirements.txt
```

## ğŸ“Š SaÃ­da

ApÃ³s a execuÃ§Ã£o, vocÃª terÃ¡:

- **Arquivo de SaÃ­da:** Um Ãºnico arquivo `.md` contendo toda a documentaÃ§Ã£o com:
  - SumÃ¡rio (Table of Contents) com links internos
  - ConteÃºdo de todas as pÃ¡ginas em Markdown
  - Links para as fontes originais
- **Arquivo de Log:** `crawler.log` com detalhes tÃ©cnicos do processo
- **Cache:** Pasta `.cache/` com pÃ¡ginas HTML baixadas (para re-execuÃ§Ãµes mais rÃ¡pidas)

### Exemplo de Resumo Final

```
============================================================
ğŸ“Š RESUMO DO CRAWLING
============================================================
âœ… PÃ¡ginas Crawleadas: 47
âŒ PÃ¡ginas Falhas: 0
ğŸ—‘ï¸  PÃ¡ginas Filtradas (junk): 2
ğŸ“ PÃ¡ginas Muito Pequenas: 3
ğŸ”— Links Encontrados: 52
ğŸŒ Links Externos (ignorados): 15
â™»ï¸  Links Duplicados (ignorados): 8
ğŸ“ Total de Caracteres: 245,892
ğŸ“– Total de Palavras: 38,421
ğŸ’¾ Tamanho do Arquivo: 189.45 KB
â±ï¸  Tempo Total: 00:02:34
============================================================
```

## ğŸ” Melhorias na VersÃ£o Atual

### ğŸ†• O que mudou?

1. **ExtraÃ§Ã£o de Links Corrigida**
   - Links agora sÃ£o extraÃ­dos ANTES de destruir tags `<nav>`, `<header>`, `<footer>`
   - Suporta links relativos e absolutos corretamente
   - NormalizaÃ§Ã£o robusta de URLs (trailing slashes, query params, fragments)

2. **LÃ³gica de DomÃ­nio Melhorada**
   - Crawleia todo o domÃ­nio base (ex: todo `docs.minimals.cc`)
   - NÃ£o mais limitado ao path inicial

3. **Logging Detalhado**
   - EstatÃ­sticas de links encontrados, externos, duplicados
   - Log de cada URL processada
   - Facilita debug quando pÃ¡ginas nÃ£o sÃ£o encontradas

4. **Filtro Conservador**
   - Remove apenas pÃ¡ginas explicitamente inÃºteis (privacy-policy, terms-of-service, cookie-policy, legal)
   - MantÃ©m cobertura completa da documentaÃ§Ã£o

5. **Melhor ConversÃ£o Markdown**
   - Suporte a `<blockquote>`
   - DetecÃ§Ã£o de `main`, `article` ou classes comuns (`content`, `main`, `body`)
   - Preserva estrutura de cÃ³digo com syntax highlighting

## âš ï¸ Avisos e Boas PrÃ¡ticas

- **Respeite robots.txt**: Use com responsabilidade e respeite os termos de serviÃ§o dos sites
- **Rate Limiting**: Se crawlear sites grandes, considere usar menos workers ou adicionar delays
- **Cache**: O cache local pode ocupar espaÃ§o em disco. Use `--clear-cache` para limpar
- **Debugging**: Se poucas pÃ¡ginas foram crawleadas, verifique `crawler.log` para entender o motivo
- **Python 3**: Sempre use `python3` e `pip3` no Linux/Mac para evitar conflitos com Python 2

## ğŸ› Troubleshooting

### Problema: "ModuleNotFoundError: No module named 'bs4'"

**SoluÃ§Ã£o:**
```bash
pip3 install beautifulsoup4 requests tqdm lxml
```

### Problema: Poucas pÃ¡ginas crawleadas

**DiagnÃ³stico:**
1. Verifique `crawler.log` para ver quais links foram encontrados
2. Teste manualmente se os links funcionam no navegador
3. Verifique se o site usa JavaScript para renderizar conteÃºdo (SPAs nÃ£o sÃ£o suportados)

**SoluÃ§Ã£o para SPAs:**
- Este crawler nÃ£o suporta sites que renderizam conteÃºdo via JavaScript (React, Vue, Angular SPAs)
- Para esses casos, considere usar Selenium ou Puppeteer

### Problema: "Permission denied" ao salvar arquivo

**SoluÃ§Ã£o:**
```bash
# Verifique permissÃµes do diretÃ³rio
ls -la

# Rode com permissÃµes adequadas ou mude o diretÃ³rio de saÃ­da
python3 crawler.py --base-url https://... --output ~/Downloads/docs.md
```

## ğŸ¤ ContribuiÃ§Ãµes

Pull requests sÃ£o bem-vindos! Fique Ã  vontade para sugerir melhorias, correÃ§Ãµes ou novas funcionalidades.

### Roadmap de Melhorias Futuras

- [ ] Suporte a autenticaÃ§Ã£o (sites que requerem login)
- [ ] ExportaÃ§Ã£o para outros formatos (PDF, HTML, EPUB)
- [ ] Suporte a SPAs (com Selenium/Puppeteer)
- [ ] Rate limiting configurÃ¡vel
- [ ] Filtros customizÃ¡veis via regex
- [ ] Modo incremental (atualizar apenas pÃ¡ginas modificadas)

## ğŸ“„ LicenÃ§a

Este projeto Ã© open source. Use livremente, mas com responsabilidade.

---

**Desenvolvido para crawlear documentaÃ§Ãµes de forma eficiente e gerar Markdown de alta qualidade.**
