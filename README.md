# Documentation Crawler

Um crawler eficiente e otimizado para baixar documenta√ß√µes de sites e convert√™-las em um √∫nico arquivo Markdown com **sum√°rio autom√°tico (TOC)**.

## ‚öôÔ∏è Funcionalidades

- üîó **Resolu√ß√£o de links relativos** e controle de dom√≠nio
- üìÇ **Convers√£o para √∫nico arquivo Markdown** com TOC
- üìù **Gera√ß√£o autom√°tica de sum√°rio (Table of Contents)**
- üõë **Timeout e retries** para lidar com falhas de rede
- üóÇ **Log detalhado** salvo em `crawler.log`
- ‚ö° **Threads para acelera√ß√£o** de downloads
- üíæ **Cache local** para evitar downloads repetidos
- üé® **Barra de progresso** com `tqdm`
- üìä **Estat√≠sticas detalhadas** ao final do processo
- üßπ **Filtro de conte√∫do** (p√°ginas muito pequenas ou irrelevantes)

## üõ†Ô∏è Requisitos

- Python 3.7+
- `pip` (geralmente vem com o Python)

## üöÄ Como Usar

### 1. Clone ou baixe o script

```bash
git clone git@github.com:bychrisr/crawler.git
cd crawler
```

### 2. (Opcional) Crie um ambiente virtual (recomendado)

Um ambiente virtual isola as depend√™ncias do script do resto do seu sistema.

```bash
# Cria o ambiente virtual (pasta 'venv')
python -m venv venv

# Ativa o ambiente virtual
# No Linux/Mac:
source venv/bin/activate
# No Windows:
venv\Scripts\activate
```

### 3. Instale as depend√™ncias

```bash
pip install requests beautifulsoup4 tqdm
```

> **Nota:** Se voc√™ ativou o ambiente virtual, as depend√™ncias ser√£o instaladas nele e n√£o afetar√£o o Python do seu sistema.

### 4. Execute o script

#### Uso B√°sico

```bash
python crawler.py --base-url <URL_DA_DOCUMENTACAO>
```

Este comando usar√° os **valores padr√£o** para todos os outros par√¢metros.

#### Exemplos Avan√ßados

**Exemplo completo com todos os par√¢metros:**

```bash
python crawler.py \
  --base-url https://tradingview.github.io/lightweight-charts/docs \
  --output minha_documentacao.md \
  --workers 2 \
  --max-pages 500 \
  --min-content-length 100 \
  --cache-dir .cache
```

**Exemplo com limpeza de cache e menos p√°ginas:**

```bash
python crawler.py \
  --base-url https://exemplo.com/docs \
  --output docs_simplificadas.md \
  --max-pages 100 \
  --clear-cache
```

## üéõÔ∏è Op√ß√µes de Comando

| Argumento | Descri√ß√£o | Valor Padr√£o |
| :--- | :--- | :--- |
| `--base-url` | **(Obrigat√≥rio)** A URL base da documenta√ß√£o a ser crawleada. | |
| `--output` | Nome do arquivo Markdown de sa√≠da. | `output.md` |
| `--workers` | N√∫mero de threads para downloads paralelos. | `2` |
| `--cache-dir` | Diret√≥rio para armazenar p√°ginas baixadas localmente. | `.cache` |
| `--max-pages` | N√∫mero m√°ximo de p√°ginas a crawlear. | `500` |
| `--min-content-length` | Tamanho m√≠nimo de conte√∫do (em caracteres) para considerar uma p√°gina v√°lida. P√°ginas menores s√£o descartadas. | `100` |
| `--clear-cache` | Limpa o diret√≥rio de cache antes de iniciar o crawling. | (flag, n√£o tem valor) |

## üìÅ Estrutura do Projeto

```
.
‚îú‚îÄ‚îÄ crawler.py          # Script principal
‚îú‚îÄ‚îÄ README.md           # Este arquivo
‚îú‚îÄ‚îÄ requirements.txt    # (opcional) Lista de depend√™ncias
‚îî‚îÄ‚îÄ venv/              # (opcional) Ambiente virtual
```

## üìù Exemplo de `requirements.txt`

Se voc√™ quiser manter um arquivo com as depend√™ncias explicitamente:

```txt
requests>=2.25.1
beautifulsoup4>=4.9.3
tqdm>=4.62.3
```

Para instalar a partir do arquivo:

```bash
pip install -r requirements.txt
```

## üìä Sa√≠da

- **Arquivo de Sa√≠da:** Um √∫nico arquivo `.md` contendo toda a documenta√ß√£o.
- **Arquivo de Log:** `crawler.log` com detalhes do processo.
- **Cache:** Pasta especificada (`--cache-dir`) com p√°ginas HTML baixadas.

Ap√≥s a conclus√£o, o script imprime um resumo com estat√≠sticas como:

- P√°ginas Crawleadas
- P√°ginas Falhas
- P√°ginas Filtradas
- Total de Caracteres e Palavras
- Tamanho do Arquivo de Sa√≠da
- Tempo Total de Execu√ß√£o

## ‚ö†Ô∏è Avisos

- Use com responsabilidade e respeite os `robots.txt` e os termos de servi√ßo dos sites.
- Baixar grandes volumes de dados pode ser intenso em recursos e tempo.
- A op√ß√£o `--max-pages` e `--min-content-length` ajudam a controlar o escopo e a qualidade da coleta.
- O cache local pode ocupar espa√ßo em disco, especialmente para sites grandes.

## ü§ù Contribui√ß√µes

Pull requests s√£o bem-vindos! Fique √† vontade para sugerir melhorias, corre√ß√µes ou novas funcionalidades.
```