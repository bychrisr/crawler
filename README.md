# Documentation Crawler v2.0.0

[![Version](https://img.shields.io/badge/version-2.0.0-blue.svg)](https://github.com/bychrisr/crawler)
[![Python](https://img.shields.io/badge/python-3.7+-green.svg)](https://www.python.org/)
[![License](https://img.shields.io/badge/license-MIT-orange.svg)](LICENSE)

Um crawler **robusto** e **profissional** para baixar documentaÃ§Ãµes de sites e convertÃª-las em um Ãºnico arquivo Markdown com sumÃ¡rio automÃ¡tico (TOC).

## ğŸ¯ Funcionalidades

### Core Features
- ğŸ”— **ResoluÃ§Ã£o robusta de links** (relativos/absolutos) com suporte a mÃºltiplos padrÃµes de documentaÃ§Ã£o
- ğŸŒ **Crawling baseado em domÃ­nio** (crawleia todo o domÃ­nio base)
- ğŸ“‚ **ConversÃ£o para Markdown** com TOC automÃ¡tico e metadados
- ğŸ¤– **Respeita robots.txt** por padrÃ£o
- ğŸ’¾ **Cache local inteligente** para re-execuÃ§Ãµes rÃ¡pidas
- âš¡ **Threads paralelas** para downloads acelerados
- ğŸ¨ **Barra de progresso** com `tqdm`

### Features AvanÃ§adas v2.0
- âœ… **ValidaÃ§Ã£o automÃ¡tica de output** (detecta problemas de extraÃ§Ã£o)
- ğŸ”„ **Retry inteligente** com exponential backoff
- â±ï¸ **Rate limiting** automÃ¡tico para evitar bans
- ğŸ” **AutenticaÃ§Ã£o HTTP** (Basic Auth)
- ğŸ“‹ **Headers HTTP customizados**
- ğŸ“Š **Metadados em JSON** salvos automaticamente
- ğŸ›‘ **Tratamento robusto de interrupÃ§Ãµes** (Ctrl+C salva progresso)
- ğŸ“ **Logging detalhado** para debug

### Melhorias de ExtraÃ§Ã£o
- ğŸ§¹ **Fallback inteligente** para sites com SSR/JavaScript
- ğŸ¯ **Filtro conservador** (remove apenas pÃ¡ginas realmente inÃºteis)
- ğŸ“Š **EstatÃ­sticas detalhadas** (cache hits, retries, links duplicados, etc.)

## ğŸ› ï¸ Requisitos

- **Python 3.7+** (use `python3` no Linux/Mac)
- `pip3` ou `pip`

## ğŸš€ InstalaÃ§Ã£o

### 1. Clone o repositÃ³rio

```bash
git clone git@github.com:bychrisr/crawler.git
cd crawler
```

### 2. (Recomendado) Crie um ambiente virtual

```bash
python3 -m venv venv

# Ativa o ambiente virtual
# Linux/Mac:
source venv/bin/activate
# Windows:
venv\Scripts\activate
```

### 3. Instale as dependÃªncias

```bash
pip install -r requirements.txt
# ou
pip3 install -r requirements.txt
```

## ğŸ“– Uso

### Uso BÃ¡sico

```bash
python3 crawler.py --base-url <URL_DA_DOCUMENTACAO>
```

### Exemplos PrÃ¡ticos

#### DocumentaÃ§Ã£o simples
```bash
python3 crawler.py \
  --base-url https://docs.minimals.cc/introduction/ \
  --output minimals-docs.md \
  --max-pages 100
```

#### DocumentaÃ§Ã£o grande com mais workers
```bash
python3 crawler.py \
  --base-url https://vuejs.org/guide/ \
  --output vue-docs.md \
  --max-pages 1000 \
  --workers 5
```

#### Com autenticaÃ§Ã£o
```bash
python3 crawler.py \
  --base-url https://docs-privadas.com/ \
  --auth-user meu-usuario \
  --auth-pass minha-senha \
  --output docs-privadas.md
```

#### Com headers customizados
```bash
python3 crawler.py \
  --base-url https://api-docs.com/ \
  --header "Authorization: Bearer TOKEN123" \
  --header "X-Custom-Header: value" \
  --output api-docs.md
```

#### Ignorar robots.txt (use com responsabilidade!)
```bash
python3 crawler.py \
  --base-url https://site.com/ \
  --no-robots \
  --output site-docs.md
```

## ğŸ›ï¸ OpÃ§Ãµes de Linha de Comando

### Argumentos Principais

| Argumento | DescriÃ§Ã£o | PadrÃ£o |
|-----------|-----------|--------|
| `--base-url` | **(ObrigatÃ³rio)** URL base da documentaÃ§Ã£o | - |
| `--output` | Arquivo de saÃ­da Markdown | `output.md` |
| `--workers` | NÃºmero de threads paralelas | `2` |
| `--max-pages` | NÃºmero mÃ¡ximo de pÃ¡ginas a crawlear | `500` |
| `--cache-dir` | DiretÃ³rio para cache local | `.cache` |
| `--min-content-length` | Tamanho mÃ­nimo de conteÃºdo (chars) | `100` |

### Flags

| Flag | DescriÃ§Ã£o |
|------|-----------|
| `--clear-cache` | Limpa o cache antes de iniciar |
| `--no-robots` | Ignora robots.txt (âš ï¸ use com cuidado) |
| `--version` | Mostra a versÃ£o do crawler |

### AutenticaÃ§Ã£o e Headers

| Argumento | DescriÃ§Ã£o |
|-----------|-----------|
| `--auth-user` | UsuÃ¡rio para autenticaÃ§Ã£o HTTP bÃ¡sica |
| `--auth-pass` | Senha para autenticaÃ§Ã£o HTTP bÃ¡sica |
| `--header` | Header HTTP customizado (pode usar mÃºltiplas vezes) |

## ğŸ“ Estrutura de SaÃ­da

ApÃ³s a execuÃ§Ã£o, vocÃª terÃ¡:

```
.
â”œâ”€â”€ output.md                 # DocumentaÃ§Ã£o em Markdown
â”œâ”€â”€ output.metadata.json      # Metadados da execuÃ§Ã£o
â”œâ”€â”€ crawler.log              # Log detalhado
â””â”€â”€ .cache/                  # Cache de pÃ¡ginas HTML
    â”œâ”€â”€ abc123def.html
    â””â”€â”€ ...
```

### Arquivo de Metadados (JSON)

Exemplo de `output.metadata.json`:

```json
{
  "version": "2.0.0",
  "base_url": "https://docs.exemplo.com/",
  "started_at": "2026-01-05T23:00:00",
  "finished_at": "2026-01-05T23:05:32",
  "config": {
    "max_workers": 3,
    "max_pages": 100,
    "min_content_length": 100,
    "respect_robots": true
  },
  "stats": {
    "fetched": 87,
    "failed": 2,
    "cache_hits": 12,
    "links_found": 95,
    "retries_performed": 4,
    ...
  }
}
```

## ğŸ“Š Exemplo de Resumo de ExecuÃ§Ã£o

```
======================================================================
ğŸ“Š RESUMO DO CRAWLING
======================================================================
âœ… PÃ¡ginas Crawleadas: 87
âŒ PÃ¡ginas Falhas: 2
ğŸ—‘ï¸  PÃ¡ginas Filtradas (junk): 3
ğŸ“ PÃ¡ginas Muito Pequenas: 1
ğŸ¤– Bloqueadas por robots.txt: 0

ğŸ”— Links Encontrados: 95
ğŸŒ Links Externos (ignorados): 234
â™»ï¸  Links Duplicados (ignorados): 1,523

ğŸ’¾ Cache Hits: 12
ğŸ”„ Retries Realizados: 4

ğŸ“ Total de Caracteres: 3,245,892
ğŸ“– Total de Palavras: 456,234
ğŸ’¾ Tamanho do Arquivo: 245,892 bytes (240.13 KB)
â±ï¸  Tempo Total: 00:05:32

======================================================================
ğŸ” VALIDAÃ‡ÃƒO DE QUALIDADE
======================================================================
âœ… Output validado com sucesso! Nenhum problema detectado.

======================================================================
ğŸ“„ Logs detalhados salvos em: crawler.log
ğŸ“Š Metadados salvos em: output.metadata.json
======================================================================
```

## ğŸ†• O Que HÃ¡ de Novo na v2.0.0

### âœ¨ Novidades Principais

1. **ValidaÃ§Ã£o AutomÃ¡tica**
   - Detecta automaticamente problemas de extraÃ§Ã£o
   - Avisa se arquivo estÃ¡ muito pequeno para o nÃºmero de pÃ¡ginas
   - Identifica baixa conversÃ£o HTML â†’ Markdown
   - Alerta sobre alta taxa de falhas

2. **Retry Inteligente**
   - Exponential backoff automÃ¡tico (2^n segundos)
   - Configurable retries (padrÃ£o: 3 tentativas)
   - EstatÃ­sticas de retries no resumo final

3. **Rate Limiting**
   - Intervalo mÃ­nimo de 0.5s entre requests
   - Evita bans e respeita servidores

4. **AutenticaÃ§Ã£o e Headers**
   - Suporte a HTTP Basic Auth
   - Headers HTTP customizados
   - Ãštil para APIs e docs privadas

5. **Metadados e Observabilidade**
   - Arquivo JSON com metadados completos
   - Timestamps de inÃ­cio/fim
   - ConfiguraÃ§Ã£o usada
   - Todas as estatÃ­sticas

6. **Tratamento de InterrupÃ§Ãµes**
   - Ctrl+C agora salva progresso parcial
   - NÃ£o perde trabalho em interrupÃ§Ãµes

7. **Robots.txt**
   - Respeita robots.txt por padrÃ£o
   - Flag `--no-robots` para ignorar (use com responsabilidade)

### ğŸ”§ Melhorias TÃ©cnicas

- ExtraÃ§Ã£o robusta para sites com SSR/JavaScript
- Fallback inteligente quando `<main>` estÃ¡ vazio
- Logs mais detalhados para debug
- CÃ³digo refatorado com type hints
- ConfiguraÃ§Ãµes centralizadas em `CrawlerConfig`

## ğŸ” Troubleshooting

### Problema: "ModuleNotFoundError"

**SoluÃ§Ã£o:**
```bash
pip3 install -r requirements.txt
```

### Problema: Poucas pÃ¡ginas crawleadas

**DiagnÃ³stico:**
1. Verifique `crawler.log`:
   ```bash
   grep "Links encontrados" crawler.log
   ```
2. Verifique se robots.txt estÃ¡ bloqueando:
   ```bash
   grep "Bloqueado por robots.txt" crawler.log
   ```
3. Teste manualmente se os links funcionam no navegador

**SoluÃ§Ãµes:**
- Use `--no-robots` se appropriate
- Aumente `--max-pages` se atingiu o limite
- Verifique se o site usa JavaScript (SPAs nÃ£o sÃ£o suportados)

### Problema: Arquivo muito pequeno (validaÃ§Ã£o alerta)

**Causas comuns:**
- Site usa JavaScript para renderizar (React/Vue SPA)
- Bloqueio por robots.txt
- Problemas na extraÃ§Ã£o de conteÃºdo

**SoluÃ§Ãµes:**
1. Verifique os avisos de validaÃ§Ã£o no resumo
2. Analise `crawler.log` para detalhes
3. Para SPAs, considere usar Selenium/Puppeteer (nÃ£o suportado atualmente)

### Problema: Alta taxa de falhas

**Causas:**
- Timeouts de rede
- Rate limiting do servidor
- Bloqueio por firewall/WAF

**SoluÃ§Ãµes:**
- Reduza `--workers` para 1-2
- Use headers customizados se necessÃ¡rio
- Verifique conectividade de rede

## âš ï¸ LimitaÃ§Ãµes Conhecidas

- **Sites com JavaScript puro (SPAs)**: Sites que renderizam 100% do conteÃºdo via JS (sem SSR) nÃ£o sÃ£o suportados. Use Selenium/Puppeteer nesses casos.
- **AutenticaÃ§Ã£o complexa**: Apenas HTTP Basic Auth Ã© suportada. OAuth e outros mÃ©todos requerem modificaÃ§Ã£o no cÃ³digo.
- **Rate Limiting agressivo**: Alguns sites podem bloquear mesmo com rate limiting. Ajuste manualmente se necessÃ¡rio.

## ğŸ¤ Contribuindo

Pull requests sÃ£o bem-vindos! Para mudanÃ§as maiores:

1. Abra uma issue primeiro para discutir
2. Fork o projeto
3. Crie uma branch (`git checkout -b feature/MinhaFeature`)
4. Commit suas mudanÃ§as (`git commit -m 'Add: MinhaFeature'`)
5. Push para a branch (`git push origin feature/MinhaFeature`)
6. Abra um Pull Request

## ğŸ“ Changelog

Veja [CHANGELOG.md](CHANGELOG.md) para histÃ³rico de versÃµes.

## ğŸ“„ LicenÃ§a

Este projeto Ã© open source sob a licenÃ§a MIT. Veja [LICENSE](LICENSE) para detalhes.

## ğŸ™ Agradecimentos

- [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/) - HTML parsing
- [requests](https://docs.python-requests.org/) - HTTP requests
- [tqdm](https://github.com/tqdm/tqdm) - Progress bars

---

**Desenvolvido com â¤ï¸ para a comunidade open source**

Para bugs, sugestÃµes ou dÃºvidas, [abra uma issue](https://github.com/bychrisr/crawler/issues)!
