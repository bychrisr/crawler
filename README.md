# Documentation Crawler v2.0.2

[![Version](https://img.shields.io/badge/version-2.0.2-blue.svg)](https://github.com/bychrisr/crawler)
[![Python](https://img.shields.io/badge/python-3.7+-green.svg)](https://www.python.org/)
[![License](https://img.shields.io/badge/license-MIT-orange.svg)](LICENSE)

Um crawler **robusto** e **profissional** para baixar documentaÃ§Ãµes de sites e convertÃª-las em um Ãºnico arquivo Markdown com sumÃ¡rio automÃ¡tico (TOC).

> **ğŸš¨ Novo na v2.0.2:** DetecÃ§Ã£o automÃ¡tica de SPAs! O crawler agora identifica sites com JavaScript e aborta gracefully em vez de travar.

> **âœ¨ v2.0.1:** ValidaÃ§Ã£o adaptativa, detecÃ§Ã£o automÃ¡tica de linguagem em code blocks, flag `--debug` e TOC inteligente!

## ğŸ¯ Funcionalidades

### Core Features
- ğŸ”— **ResoluÃ§Ã£o robusta de links** (relativos/absolutos) com suporte a mÃºltiplos padrÃµes de documentaÃ§Ã£o
- ğŸŒ **Crawling baseado em domÃ­nio** (crawleia todo o domÃ­nio base)
- ğŸ“‚ **ConversÃ£o para Markdown** com TOC automÃ¡tico e metadados
- ğŸ¤– **Respeita robots.txt** por padrÃ£o
- ğŸ’¾ **Cache local inteligente** para re-execuÃ§Ãµes rÃ¡pidas
- âš¡ **Threads paralelas** para downloads acelerados
- ğŸ¨ **Barra de progresso** com `tqdm`

### Features AvanÃ§adas v2.0
- âœ… **ValidaÃ§Ã£o automÃ¡tica de output** (detecta problemas de extraÃ§Ã£o)
- ğŸ”„ **Retry inteligente** com exponential backoff
- â±ï¸ **Rate limiting** automÃ¡tico para evitar bans
- ğŸ” **AutenticaÃ§Ã£o HTTP** (Basic Auth)
- ğŸ“‹ **Headers HTTP customizados**
- ğŸ“Š **Metadados em JSON** salvos automaticamente
- ğŸ›‘ **Tratamento robusto de interrupÃ§Ãµes** (Ctrl+C salva progresso)
- ğŸ“ **Logging detalhado** para debug

### Melhorias de ExtraÃ§Ã£o
- ğŸ§¹ **Fallback inteligente** para sites com SSR/JavaScript
- ğŸ¯ **Filtro conservador** (remove apenas pÃ¡ginas realmente inÃºteis)
- ğŸ“Š **EstatÃ­sticas detalhadas** (cache hits, retries, links duplicados, etc.)

## ğŸ› ï¸ Requisitos

- **Python 3.7+** (use `python3` no Linux/Mac)
- `pip3` ou `pip`

## ğŸš€ InstalaÃ§Ã£o

### 1. Clone o repositÃ³rio

```bash
git clone git@github.com:bychrisr/crawler.git
cd crawler
```

### 2. (Recomendado) Crie um ambiente virtual

```bash
python3 -m venv venv

# Ativa o ambiente virtual
# Linux/Mac:
source venv/bin/activate
# Windows:
venv\Scripts\activate
```

### 3. Instale as dependÃªncias

```bash
pip install -r requirements.txt
# ou
pip3 install -r requirements.txt
```

## ğŸ“– Uso

### Uso BÃ¡sico

```bash
python3 crawler.py --base-url <URL_DA_DOCUMENTACAO>
```

### Exemplos PrÃ¡ticos

#### DocumentaÃ§Ã£o simples
```bash
python3 crawler.py \
  --base-url https://docs.minimals.cc/introduction/ \
  --output minimals-docs.md \
  --max-pages 100
```

#### DocumentaÃ§Ã£o grande com mais workers
```bash
python3 crawler.py \
  --base-url https://vuejs.org/guide/ \
  --output vue-docs.md \
  --max-pages 1000 \
  --workers 5
```

#### Com autenticaÃ§Ã£o
```bash
python3 crawler.py \
  --base-url https://docs-privadas.com/ \
  --auth-user meu-usuario \
  --auth-pass minha-senha \
  --output docs-privadas.md
```

#### Com headers customizados
```bash
python3 crawler.py \
  --base-url https://api-docs.com/ \
  --header "Authorization: Bearer TOKEN123" \
  --header "X-Custom-Header: value" \
  --output api-docs.md
```

#### Ignorar robots.txt (use com responsabilidade!)
```bash
python3 crawler.py \
  --base-url https://site.com/ \
  --no-robots \
  --output site-docs.md
```

## ğŸ›ï¸ OpÃ§Ãµes de Linha de Comando

### Argumentos Principais

| Argumento | DescriÃ§Ã£o | PadrÃ£o |
|-----------|-----------|--------|
| `--base-url` | **(ObrigatÃ³rio)** URL base da documentaÃ§Ã£o | - |
| `--output` | Arquivo de saÃ­da Markdown | `output.md` |
| `--workers` | NÃºmero de threads paralelas | `2` |
| `--max-pages` | NÃºmero mÃ¡ximo de pÃ¡ginas a crawlear | `500` |
| `--cache-dir` | DiretÃ³rio para cache local | `.cache` |
| `--min-content-length` | Tamanho mÃ­nimo de conteÃºdo (chars) | `100` |

### Flags

| Flag | DescriÃ§Ã£o |
|------|-----------|
| `--clear-cache` | Limpa o cache antes de iniciar |
| `--no-robots` | Ignora robots.txt (âš ï¸ use com cuidado) |
| `--debug` | Mostra extraÃ§Ã£o de conteÃºdo em tempo real (Ãºtil para debug) |
| `--version` | Mostra a versÃ£o do crawler |

### AutenticaÃ§Ã£o e Headers

| Argumento | DescriÃ§Ã£o |
|-----------|-----------|
| `--auth-user` | UsuÃ¡rio para autenticaÃ§Ã£o HTTP bÃ¡sica |
| `--auth-pass` | Senha para autenticaÃ§Ã£o HTTP bÃ¡sica |
| `--header` | Header HTTP customizado (pode usar mÃºltiplas vezes) |

## ğŸ“ Estrutura de SaÃ­da

ApÃ³s a execuÃ§Ã£o, vocÃª terÃ¡:

```
.
â”œâ”€â”€ output.md                 # DocumentaÃ§Ã£o em Markdown
â”œâ”€â”€ output.metadata.json      # Metadados da execuÃ§Ã£o
â”œâ”€â”€ crawler.log              # Log detalhado
â””â”€â”€ .cache/                  # Cache de pÃ¡ginas HTML
    â”œâ”€â”€ abc123def.html
    â””â”€â”€ ...
```

### Arquivo de Metadados (JSON)

Exemplo de `output.metadata.json`:

```json
{
  "version": "2.0.0",
  "base_url": "https://docs.exemplo.com/",
  "started_at": "2026-01-05T23:00:00",
  "finished_at": "2026-01-05T23:05:32",
  "config": {
    "max_workers": 3,
    "max_pages": 100,
    "min_content_length": 100,
    "respect_robots": true
  },
  "stats": {
    "fetched": 87,
    "failed": 2,
    "cache_hits": 12,
    "links_found": 95,
    "retries_performed": 4,
    ...
  }
}
```

## ğŸ“Š Exemplo de Resumo de ExecuÃ§Ã£o

```
======================================================================
ğŸ“Š RESUMO DO CRAWLING
======================================================================
âœ… PÃ¡ginas Crawleadas: 87
âŒ PÃ¡ginas Falhas: 2
ğŸ—‘ï¸  PÃ¡ginas Filtradas (junk): 3
ğŸ“ PÃ¡ginas Muito Pequenas: 1
ğŸ¤– Bloqueadas por robots.txt: 0

ğŸ”— Links Encontrados: 95
ğŸŒ Links Externos (ignorados): 234
â™»ï¸  Links Duplicados (ignorados): 1,523

ğŸ’¾ Cache Hits: 12
ğŸ”„ Retries Realizados: 4

ğŸ“ Total de Caracteres: 3,245,892
ğŸ“– Total de Palavras: 456,234
ğŸ’¾ Tamanho do Arquivo: 245,892 bytes (240.13 KB)
â±ï¸  Tempo Total: 00:05:32

======================================================================
ğŸ” VALIDAÃ‡ÃƒO DE QUALIDADE
======================================================================
âœ… Output validado com sucesso! Nenhum problema detectado.

======================================================================
ğŸ“„ Logs detalhados salvos em: crawler.log
ğŸ“Š Metadados salvos em: output.metadata.json
======================================================================
```

## ğŸ†• O Que HÃ¡ de Novo na v2.0.2

### ğŸš¨ DetecÃ§Ã£o AutomÃ¡tica de SPA

O crawler agora **detecta automaticamente** sites que usam JavaScript puro (SPAs) e **aborta gracefully** em vez de travar!

**Problema que resolve:**
```bash
# Antes (v2.0.1):
python3 crawler.py --base-url https://spa-site.com/
# â†’ Trava em loop infinito ou deadlock ğŸ”’

# Agora (v2.0.2):
python3 crawler.py --base-url https://spa-site.com/
# â†’ Detecta e aborta com mensagem clara âœ…
```

**Output quando SPA Ã© detectada:**
```
======================================================================
âš ï¸  SPA DETECTADA!
======================================================================
O site https://minimals.cc/components/ parece ser uma SPA pura
(Single Page Application - React/Vue/Angular).

SPAs renderizam conteÃºdo via JavaScript, que este
crawler nÃ£o executa. O HTML retornado estÃ¡ quase vazio.

ğŸ“Š AnÃ¡lise:
  - Tamanho HTML: 494 bytes
  - Links encontrados: 0
  - ConteÃºdo textual: 15 chars

ğŸ’¡ SoluÃ§Ãµes:
  1. Use a versÃ£o de documentaÃ§Ã£o (docs.exemplo.com)
  2. Use Selenium/Puppeteer para SPAs
  3. Verifique se existe versÃ£o SSR do site
======================================================================

âŒ Crawling abortado: SPA detectada
```

### â±ï¸ Timeout de Inatividade

ProteÃ§Ã£o contra crawlers travados:

```
Se sem progresso por 30s:
âš ï¸  Timeout: Sem progresso por 30s
   PÃ¡ginas crawleadas: 5
   PossÃ­veis causas:
   - Site Ã© uma SPA (JavaScript puro)
   - Problemas de rede
   - Site bloqueia crawlers

ğŸ’¾ Salvando progresso parcial...
```

### ğŸ”¬ Como Identificar uma SPA

**Sites que NÃƒO funcionam (SPAs):**
- Create React App sem SSR
- Vue CLI sem SSR
- Angular sem Universal
- Sites com `<div id="root"></div>` vazio

**Sites que funcionam:**
- Next.js (SSR/SSG)
- Nuxt (SSR/SSG)
- Gatsby (SSG)
- Sites estÃ¡ticos (HTML puro)
- Docusaurus, VuePress, MkDocs

**Teste rÃ¡pido:**
```bash
# View Source no navegador
# Se vÃª conteÃºdo â†’ âœ… Funciona
# Se sÃ³ vÃª <div id="root"> â†’ âŒ SPA
```

---

## ğŸ†• O Que HÃ¡ de Novo na v2.0.1

### ğŸ¯ ValidaÃ§Ã£o Inteligente

A v2.0.1 resolve o principal problema da v2.0.0: **avisos falsos-positivos** sobre "baixa conversÃ£o".

**Antes (v2.0.0):**
```
âš ï¸ Baixa conversÃ£o de HTML para Markdown: 0.9% (esperado: >30%)
```
âŒ Aparecia em 100% dos sites modernos, mesmo funcionando corretamente!

**Agora (v2.0.1):**
```
âœ… Output validado com sucesso! Nenhum problema detectado.
```
âœ… Threshold se adapta automaticamente ao tipo de site:
- Sites de docs tÃ©cnicas (muito cÃ³digo): 1%
- Sites pequenos: 3%
- Sites modernos padrÃ£o: 1.5%

### ğŸ”¬ DetecÃ§Ã£o AutomÃ¡tica de Linguagem

Code blocks agora tÃªm syntax highlighting correto!

**Antes:**
````markdown
```text
import React from 'react';
const App = () => <div>Hello</div>;
```
````

**Agora:**
````markdown
```javascript
import React from 'react';
const App = () => <div>Hello</div>;
```
````

Linguagens detectadas: JavaScript, TypeScript, JSX, TSX, Python, Bash, JSON, CSS, HTML, Markdown

### ğŸ› Flag `--debug`

Novo modo para debugar extraÃ§Ã£o:

```bash
python3 crawler.py --base-url https://docs.exemplo.com/ --debug

# Output:
[DEBUG] ExtraÃ­do de https://docs.exemplo.com/intro:
  - 8 headers
  - 15 parÃ¡grafos
  - 12 items de lista
  - 5 code blocks
```

### ğŸ“‹ TOC Mais Limpo

PÃ¡ginas vazias (root sem tÃ­tulo) nÃ£o aparecem mais no Table of Contents.

---

## ğŸ†• O Que HÃ¡ de Novo na v2.0.0

### âœ¨ Novidades Principais

1. **ValidaÃ§Ã£o AutomÃ¡tica**
   - Detecta automaticamente problemas de extraÃ§Ã£o
   - Avisa se arquivo estÃ¡ muito pequeno para o nÃºmero de pÃ¡ginas
   - Identifica baixa conversÃ£o HTML â†’ Markdown
   - Alerta sobre alta taxa de falhas

2. **Retry Inteligente**
   - Exponential backoff automÃ¡tico (2^n segundos)
   - Configurable retries (padrÃ£o: 3 tentativas)
   - EstatÃ­sticas de retries no resumo final

3. **Rate Limiting**
   - Intervalo mÃ­nimo de 0.5s entre requests
   - Evita bans e respeita servidores

4. **AutenticaÃ§Ã£o e Headers**
   - Suporte a HTTP Basic Auth
   - Headers HTTP customizados
   - Ãštil para APIs e docs privadas

5. **Metadados e Observabilidade**
   - Arquivo JSON com metadados completos
   - Timestamps de inÃ­cio/fim
   - ConfiguraÃ§Ã£o usada
   - Todas as estatÃ­sticas

6. **Tratamento de InterrupÃ§Ãµes**
   - Ctrl+C agora salva progresso parcial
   - NÃ£o perde trabalho em interrupÃ§Ãµes

7. **Robots.txt**
   - Respeita robots.txt por padrÃ£o
   - Flag `--no-robots` para ignorar (use com responsabilidade)

### ğŸ”§ Melhorias TÃ©cnicas

- ExtraÃ§Ã£o robusta para sites com SSR/JavaScript
- Fallback inteligente quando `<main>` estÃ¡ vazio
- Logs mais detalhados para debug
- CÃ³digo refatorado com type hints
- ConfiguraÃ§Ãµes centralizadas em `CrawlerConfig`

## ğŸ” Troubleshooting

### Problema: "ModuleNotFoundError"

**SoluÃ§Ã£o:**
```bash
pip3 install -r requirements.txt
```

### Problema: Poucas pÃ¡ginas crawleadas

**DiagnÃ³stico:**
1. Verifique `crawler.log`:
   ```bash
   grep "Links encontrados" crawler.log
   ```
2. Verifique se robots.txt estÃ¡ bloqueando:
   ```bash
   grep "Bloqueado por robots.txt" crawler.log
   ```
3. Teste manualmente se os links funcionam no navegador

**SoluÃ§Ãµes:**
- Use `--no-robots` se appropriate
- Aumente `--max-pages` se atingiu o limite
- Verifique se o site usa JavaScript (SPAs nÃ£o sÃ£o suportados)

### Problema: Arquivo muito pequeno (validaÃ§Ã£o alerta)

**Causas comuns:**
- Site usa JavaScript para renderizar (React/Vue SPA)
- Bloqueio por robots.txt
- Problemas na extraÃ§Ã£o de conteÃºdo

**SoluÃ§Ãµes:**
1. Verifique os avisos de validaÃ§Ã£o no resumo
2. Analise `crawler.log` para detalhes
3. Para SPAs, considere usar Selenium/Puppeteer (nÃ£o suportado atualmente)

### Problema: Alta taxa de falhas

**Causas:**
- Timeouts de rede
- Rate limiting do servidor
- Bloqueio por firewall/WAF

**SoluÃ§Ãµes:**
- Reduza `--workers` para 1-2
- Use headers customizados se necessÃ¡rio
- Verifique conectividade de rede

## âš ï¸ LimitaÃ§Ãµes Conhecidas

### ğŸš¨ SPAs (Single Page Applications) - v2.0.2+

**NÃ£o suportado:** Sites que renderizam 100% do conteÃºdo via JavaScript.

A partir da v2.0.2, o crawler **detecta automaticamente** SPAs e aborta com mensagem clara.

| Framework | SSR? | Funciona? | Exemplo |
|-----------|------|-----------|---------|
| **Create React App** | âŒ NÃ£o | âŒ NÃ£o | `minimals.cc/components` |
| **Next.js** | âœ… Sim | âœ… Sim | `docs.minimals.cc` |
| **Vue CLI** | âŒ NÃ£o | âŒ NÃ£o | Sites Vue sem Nuxt |
| **Nuxt** | âœ… Sim | âœ… Sim | Sites Nuxt com SSR |
| **Angular** | âŒ NÃ£o* | âŒ NÃ£o* | *Sem Universal |
| **Gatsby** | âœ… SSG | âœ… Sim | Sites estÃ¡ticos |
| **Docusaurus** | âœ… SSG | âœ… Sim | DocumentaÃ§Ãµes |
| **VuePress** | âœ… SSG | âœ… Sim | DocumentaÃ§Ãµes |
| **MkDocs** | âœ… SSG | âœ… Sim | DocumentaÃ§Ãµes |

**Como identificar uma SPA:**
1. Abra "View Source" (Ctrl+U) no navegador
2. Se vÃª apenas `<div id="root"></div>` vazio â†’ SPA âŒ
3. Se vÃª conteÃºdo HTML completo â†’ SSR/SSG âœ…

**SoluÃ§Ãµes para SPAs:**
- Procure versÃ£o de documentaÃ§Ã£o (geralmente usa SSG)
- Use Selenium/Puppeteer (nÃ£o incluÃ­do)
- Verifique se o site tem versÃ£o SSR

### Outras LimitaÃ§Ãµes

- **AutenticaÃ§Ã£o complexa**: Apenas HTTP Basic Auth Ã© suportada. OAuth e outros mÃ©todos requerem modificaÃ§Ã£o no cÃ³digo.
- **Rate Limiting agressivo**: Alguns sites podem bloquear mesmo com rate limiting. Ajuste `--workers` se necessÃ¡rio.
- **JavaScript Interativo**: Sites que requerem cliques/interaÃ§Ãµes nÃ£o sÃ£o suportados.

## ğŸ¤ Contribuindo

Pull requests sÃ£o bem-vindos! Para mudanÃ§as maiores:

1. Abra uma issue primeiro para discutir
2. Fork o projeto
3. Crie uma branch (`git checkout -b feature/MinhaFeature`)
4. Commit suas mudanÃ§as (`git commit -m 'Add: MinhaFeature'`)
5. Push para a branch (`git push origin feature/MinhaFeature`)
6. Abra um Pull Request

## ğŸ“ Changelog

Veja [CHANGELOG.md](CHANGELOG.md) para histÃ³rico de versÃµes.

## ğŸ“„ LicenÃ§a

Este projeto Ã© open source sob a licenÃ§a MIT. Veja [LICENSE](LICENSE) para detalhes.

## ğŸ™ Agradecimentos

- [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/) - HTML parsing
- [requests](https://docs.python-requests.org/) - HTTP requests
- [tqdm](https://github.com/tqdm/tqdm) - Progress bars

---

**Desenvolvido com â¤ï¸ para a comunidade open source**

Para bugs, sugestÃµes ou dÃºvidas, [abra uma issue](https://github.com/bychrisr/crawler/issues)!
